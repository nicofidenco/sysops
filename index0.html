<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS SysOps Administrator - Practice Quiz</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
        }
        .explanation-content {
            transition: max-height 0.5s ease-in-out, opacity 0.5s ease-in-out;
            max-height: 0;
            opacity: 0;
            overflow: hidden;
        }
        .explanation-content.show {
            max-height: 1500px; /* Increased for potentially long explanations */
            opacity: 1;
        }
        .option-btn {
            transition: background-color 0.3s, border-color 0.3s, color 0.3s;
        }
        .correct {
            background-color: #10B981 !important;
            border-color: #059669 !important;
            color: white !important;
        }
        .incorrect {
            background-color: #EF4444 !important;
            border-color: #DC2626 !important;
            color: white !important;
        }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <div class="container mx-auto p-4 sm:p-6 lg:p-8 max-w-4xl">

        <header class="bg-white shadow-md rounded-lg p-4 mb-6 flex justify-between items-center flex-wrap gap-4">
            <div>
                <h1 class="text-xl sm:text-2xl font-bold text-gray-700">AWS SysOps Practice Quiz</h1>
                <p id="score" class="text-lg font-semibold text-blue-600">Score: 0 / 0</p>
            </div>
            <div class="flex flex-col items-end gap-2">
                 <div class="flex items-center space-x-2">
                    <span id="timer" class="text-sm font-mono bg-gray-200 px-2 py-1 rounded">00:00:00</span>
                    <button id="start-timer" class="text-xs bg-green-500 hover:bg-green-600 text-white font-bold py-1 px-2 rounded">Start</button>
                    <button id="stop-timer" class="text-xs bg-yellow-500 hover:bg-yellow-600 text-white font-bold py-1 px-2 rounded">Stop</button>
                    <button id="reset-timer" class="text-xs bg-red-500 hover:bg-red-600 text-white font-bold py-1 px-2 rounded">Reset</button>
                </div>
                <button id="reset-quiz" class="text-xs bg-indigo-500 hover:bg-indigo-600 text-white font-bold py-1 px-2 rounded">Reset Quiz</button>
            </div>
        </header>

        <main id="quiz-container" class="bg-white shadow-lg rounded-lg p-6 min-h-[400px]">
            <!-- Quiz content will be injected here -->
        </main>

        <footer class="mt-6 flex justify-between items-center">
            <button id="prev-btn" class="text-xs bg-gray-300 hover:bg-gray-400 text-gray-800 font-bold py-1 px-3 rounded-lg disabled:opacity-50" disabled>Previous</button>
            <div class="flex items-center">
                <label for="question-select" class="text-sm mr-2">Go to:</label>
                <select id="question-select" class="text-xs border border-gray-300 rounded-md p-1"></select>
            </div>
            <button id="next-btn" class="text-xs bg-gray-300 hover:bg-gray-400 text-gray-800 font-bold py-1 px-3 rounded-lg">Next</button>
        </footer>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            // --- DATA & STATE ---
            let currentQuestionIndex = 0;
            let timerInterval;
            let timerSeconds = 0;

            const quizContainer = document.getElementById('quiz-container');
            const scoreEl = document.getElementById('score');
            const prevBtn = document.getElementById('prev-btn');
            const nextBtn = document.getElementById('next-btn');
            const questionSelect = document.getElementById('question-select');
            const resetQuizBtn = document.getElementById('reset-quiz');
            
            // Timer elements
            const timerDisplay = document.getElementById('timer');
            const startTimerBtn = document.getElementById('start-timer');
            const stopTimerBtn = document.getElementById('stop-timer');
            const resetTimerBtn = document.getElementById('reset-timer');

            // --- STRUCTURED QUESTIONS ARRAY ---
            const questions = [
                {
                    number: 1,
                    title: "EC2 Connectivity Timeout (Based on #339)",
                    scenario: "A SysOps administrator launches a new Amazon EC2 Linux instance into a public subnet. The instance is running, and the administrator has its public IP address. However, every attempt to connect remotely (e.g., via SSH) results in a connection timeout error.",
                    questionText: "Which action will allow the SysOps administrator to remotely connect to the instance?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Add a route table entry in the public subnet for the SysOps administrator's IP address." },
                        { letter: "B", text: "Add an outbound network ACL rule to allow TCP port 22 for the SysOps administrator's IP address." },
                        { letter: "C", text: "Modify the instance security group to allow inbound SSH traffic from the SysOps administrator's IP address." },
                        { letter: "D", text: "Modify the instance security group to allow outbound SSH traffic to the SysOps administrator's IP address." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "A connection timeout error is a classic sign that a firewall is blocking the traffic before it can reach the destination. In AWS, the primary firewall protecting an EC2 instance is its Security Group. By default, security groups deny all inbound traffic. To connect to a Linux instance using SSH, you must explicitly allow inbound traffic on TCP port 22. Modifying the security group to add an inbound rule for port 22 from the administrator's specific IP address is the correct and most secure solution.",
                    wrongExplanation: "A: Route tables control the flow of traffic between subnets and to destinations outside the VPC (like the internet via an Internet Gateway). They do not filter traffic to a specific instance based on port or IP. The instance is in a public subnet, which should already have a route to the Internet Gateway.\n\n**B:** This is incorrect for two reasons. First, the problem is with inbound traffic to the instance, not outbound. Second, Network ACLs are stateless, but they are less commonly the cause of initial connection issues than security groups. The default Network ACL allows all traffic.\n\n**D:** The connection attempt is an inbound request to the EC2 instance. An outbound rule controls traffic leaving the instance. While outbound rules are important, they are not the cause of this specific problem."
                },
                {
                    number: 2,
                    title: "Application & Lambda Routing (Based on #338)",
                    scenario: "A company is transitioning a web application from Amazon EC2 instances to an AWS Lambda function. During the migration, they need to route traffic based on the URL path. For example, requests to /api/v1/... should go to the old EC2 instances, while requests to /api/v2/... should go to the new Lambda function.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Configure a Gateway Load Balancer." },
                        { letter: "B", text: "Configure a Network Load Balancer." },
                        { letter: "C", text: "Configure a Network Load Balancer with a regular expression." },
                        { letter: "D", text: "Configure an Application Load Balancer." }
                    ],
                    correctAnswers: ["D"],
                    explanation: "The key requirement is path-based routing, which is a feature of the application layer (Layer 7) of the OSI model. An Application Load Balancer (ALB) operates at Layer 7 and is designed for this exact purpose. It can inspect the content of the request, including the URL path, and route traffic to different target groups based on rules you define. In this case, you would create one target group for the EC2 instances and another for the Lambda function, with listener rules on the ALB to direct traffic based on the /api/v1 or /api/v2 path.",
                    wrongExplanation: "A: A Gateway Load Balancer is a specialized service used to deploy, scale, and manage third-party virtual network appliances (like firewalls or intrusion detection systems). It operates at Layer 3 (the network layer) and cannot perform path-based routing.\n\n**B & C:** A Network Load Balancer (NLB) operates at Layer 4 (the transport layer). It is extremely fast but makes routing decisions based on information like IP address, port, and protocol. It is not aware of application-level content like URL paths and therefore cannot be used for this scenario."
                },
                {
                    number: 3,
                    title: "Auto Scaling Cooldown Period (Based on #336)",
                    scenario: "A SysOps administrator has an Auto Scaling group using a simple scaling policy based on the RequestCountPerTarget metric. The administrator observes that the metric threshold was breached twice within a 180-second period. The Auto Scaling group is using default settings.",
                    questionText: "How will the number of EC2 instances in this Auto Scaling group be affected in this scenario?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "The Auto Scaling group will launch an additional EC2 instance every time the RequestCountPerTarget metric exceeds the predefined limit." },
                        { letter: "B", text: "The Auto Scaling group will launch one EC2 instance and will wait for the default cooldown period before launching another instance." },
                        { letter: "C", text: "The Auto Scaling group will send an alert to the ALB to rebalance the traffic and not add new EC2 instances until the load is normalized." },
                        { letter: "D", text: "The Auto Scaling group will try to distribute the traffic among all EC2 instances before launching another instance." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "Simple scaling policies have a cooldown period to prevent the Auto Scaling group from launching or terminating additional instances before the effects of a previous scaling activity are visible. The default cooldown period is 300 seconds (5 minutes). When the first alarm triggers, the Auto Scaling group launches a new instance and enters the cooldown period. Even though the alarm triggers again 180 seconds later, the group will ignore it because it is still within the 300-second cooldown period. It will not launch a second instance until the cooldown expires.",
                    wrongExplanation: "A: This describes behavior without a cooldown period, which is incorrect for simple scaling policies. The cooldown period is specifically designed to prevent this kind of rapid, potentially excessive scaling.\n\n**C:** The Auto Scaling group's primary function is to adjust the number of instances, not to directly instruct the ALB to rebalance traffic. The ALB will automatically rebalance traffic as new instances become healthy.\n\n**D:** The ALB is responsible for distributing traffic. The Auto Scaling group's role is to add or remove instances based on the scaling policy."
                },
                               
                
               
                {
                    number: 4,
                    title: "Database Failover (Based on #335)",
                    scenario: "A company's application uses a single Amazon RDS DB instance. They are concerned about the lack of a failover solution and need to implement one that is automatic and does not lose any committed transactions in the event of a disaster.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an RDS read replica in the same AWS Region." },
                        { letter: "B", text: "Create an RDS read replica in a different AWS Region." },
                        { letter: "C", text: "Modify the DB instance to be a Multi-AZ deployment." },
                        { letter: "D", text: "Set up a CloudWatch alarm to restart the DB instance if memory utilization is high." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "The requirements are for automatic failover and no data loss (zero RPO - Recovery Point Objective). The AWS feature designed specifically for this is an RDS Multi-AZ deployment. When you enable Multi-AZ, RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone (AZ). All database writes are synchronously replicated to the standby. If the primary database fails, RDS automatically fails over to the standby replica without any manual intervention and without losing any committed data.",
                    wrongExplanation: "A & B: Read replicas are primarily for scaling read traffic, not for high availability. They use asynchronous replication, which means there is a small delay (replication lag). In a failover, any data committed to the primary that hasn't yet been replicated to the replica would be lost. Furthermore, promoting a read replica to be the new primary is a manual process (or requires custom automation), not an automatic one.\n\n**D:** This is a monitoring solution, not a high-availability or failover solution. Restarting an instance due to high memory usage does not protect against an underlying host or AZ failure and would cause downtime."
                },
                {
                    number: 5,
                    title: "Centralized Policy Enforcement (Based on #334)",
                    scenario: "A company uses AWS Organizations with separate Organizational Units (OUs) for production and development. A corporate policy dictates that developers can only use a specific list of approved AWS services within the production account.",
                    questionText: "What is the MOST operationally efficient solution to control the production account?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create a customer managed policy in AWS Identity and Access Management (IAM). Apply the policy to all users within the production account." },
                        { letter: "B", text: "Create a job function policy in AWS Identity and Access Management (IAM). Apply the policy to all users within the production OU." },
                        { letter: "C", text: "Create a service control policy (SCP). Apply the SCP to the production OU." },
                        { letter: "D", text: "Create an IAM policy. Apply the policy in Amazon API Gateway to restrict the production account." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "Service Control Policies (SCPs) are a feature of AWS Organizations designed for this exact purpose. SCPs offer central control over the maximum available permissions for all accounts in your organization. By attaching an SCP to the production OU that explicitly denies access to all services except the approved ones, you create a preventative guardrail. This policy applies to all IAM users and roles in every account within that OU, including the root user, ensuring consistent enforcement with maximum operational efficiency.",
                    wrongExplanation: "A & B: Using IAM policies is less efficient. You would need to ensure that every single IAM user and role in the production account has this policy attached. It's easy for a new user or role to be created without the policy, leading to a security gap. SCPs provide a top-down enforcement that cannot be overridden by IAM administrators within the account.\n\n**D:** Amazon API Gateway is a service for creating, publishing, and securing APIs. It has no capability to enforce broad service-level permissions for an entire AWS account."
                },
                {
                    number: 6,
                    title: "Automating Service Quota Increases (Based on #332)",
                    scenario: "A company wants to monitor the number of running EC2 instances and automatically request a service quota increase when the count approaches the current limit.",
                    questionText: "Which solution meets these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an Amazon CloudWatch alarm to monitor Service Quotas. Configure the alarm to invoke an AWS Lambda function to request a quota increase when the alarm reaches the threshold." },
                        { letter: "B", text: "Create an AWS Config rule to monitor Service Quotas." },
                        { letter: "C", text: "Create an Amazon CloudWatch alarm to monitor the AWS Health Dashboard." },
                        { letter: "D", text: "Create an Amazon CloudWatch alarm to monitor AWS Trusted Advisor service quotas. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic to increase the quota." }
                    ],
                    correctAnswers: ["A"],
                    explanation: "This solution provides a complete, automated workflow. Service Quotas integrates with CloudWatch, allowing you to create alarms based on your usage of a service relative to its quota. When the CloudWatch alarm enters the ALARM state, it can be configured to trigger an AWS Lambda function. This function can then use the AWS SDK to programmatically call the RequestServiceQuotaIncrease API action, fully automating the process.",
                    wrongExplanation: "B: AWS Config is used to assess, audit, and evaluate the configurations of your AWS resources. It does not monitor usage metrics against quotas.\n\n**C:** The AWS Health Dashboard provides information about service health and planned events, not your specific resource usage against quotas.\n\n**D:** While Trusted Advisor does check for service limits, and you can create alarms from it, an SNS topic by itself cannot perform an action like requesting a quota increase. SNS is a pub/sub messaging service; it can notify you or trigger other services (like Lambda), but it doesn't have the logic to make an API call to increase a quota. The Lambda function in option A is the missing piece that provides the necessary action."
                },
                {
                    number: 7,
                    title: "Retaining Resources on Stack Deletion (Based on #331)",
                    scenario: "A SysOps administrator uses AWS CloudFormation to manage a stack of EC2 instances. The administrator needs to ensure that if the CloudFormation stack is deleted, the EC2 instances and all their associated data (on their EBS volumes) are preserved.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Set the DeletionPolicy attribute to Snapshot for the EC2 instance resource in the CloudFormation template." },
                        { letter: "B", text: "Automate backups by using Amazon Data Lifecycle Manager (Amazon DLM)." },
                        { letter: "C", text: "Create a backup plan in AWS Backup." },
                        { letter: "D", text: "Set the DeletionPolicy attribute to Retain for the EC2 instance resource in the CloudFormation template." }
                    ],
                    correctAnswers: ["D"],
                    explanation: "The DeletionPolicy is a CloudFormation resource attribute that tells CloudFormation what to do with a resource when its stack is deleted. By default, most resources are deleted. Setting DeletionPolicy: Retain instructs CloudFormation to leave the resource intact when the stack is deleted. This is the direct, built-in mechanism to achieve the desired outcome of keeping the EC2 instance and its data.",
                    wrongExplanation: "A: The Snapshot deletion policy applies to resources that support snapshots, like AWS::EC2::Volume (EBS volumes) and AWS::RDS::DBInstance. It does not apply directly to the AWS::EC2::Instance resource itself. While it would save the data on a volume, it would not save the instance.\n\n**B & C:** AWS Backup and DLM are excellent services for creating backups and snapshots for disaster recovery and data protection. However, they do not prevent CloudFormation from deleting the original EC2 instance when the stack is deleted. They only provide a way to restore the data later. The requirement is to keep the original instance."
                },
                {
                    number: 8,
                    title: "Database Recovery and Efficiency (Based on #330)",
                    scenario: "A company runs a MySQL database on a single EC2 instance. A SysOps administrator needs to find the MOST operationally efficient solution to minimize both potential data loss and recovery time in case of a database failure.",
                    questionText: "What is the MOST operationally efficient solution that meets these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create a CloudWatch alarm to stop and start the EC2 instance on failure." },
                        { letter: "B", text: "Create an Amazon RDS for MySQL Multi-AZ DB instance. Use a MySQL native backup that is stored in Amazon S3 to restore the data to the new database. Update the connection string in the web application." },
                        { letter: "C", text: "Create an Amazon RDS for MySQL Single-AZ DB instance with a read replica." },
                        { letter: "D", text: "Use Amazon Data Lifecycle Manager (Amazon DLM) to take an hourly snapshot of the EBS volume." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "Migrating the database from a self-managed EC2 instance to Amazon RDS for MySQL with Multi-AZ is the most operationally efficient and robust solution. RDS is a managed service, which offloads operational tasks like patching, backups, and failover. The Multi-AZ feature provides a hot standby in a different Availability Zone with synchronous replication, ensuring minimal data loss (RPO near zero) and fast, automatic failover (low RTO). This directly addresses both requirements with the least ongoing administrative effort. The one-time effort of migrating and updating the connection string is far outweighed by the long-term operational benefits.",
                    wrongExplanation: "A: Stopping and starting an instance might resolve a software issue but does nothing to protect against an underlying hardware or AZ failure. It's not a high-availability solution.\n\n**C:** A read replica uses asynchronous replication, which can lead to data loss during a failover. Promoting a read replica is also a manual process, increasing recovery time and operational overhead.\n\n**D:** While DLM automates snapshots, this solution has a higher RPO (up to one hour of data loss) and a much higher RTO. Restoring from a snapshot involves creating a new volume and attaching it to a new instance, a manual and time-consuming process compared to the automatic failover of RDS Multi-AZ."
                },
                {
                    number: 9,
                    title: "Stopping Idle Instances (Based on #329)",
                    scenario: "A SysOps administrator needs to implement a cost-saving solution to automatically stop development EC2 instances when they are not in use. An instance is considered \"not in use\" if its average CPU utilization is lower than 5% for 30 minutes.",
                    questionText: "Which solution will meet this requirement?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Assess AWS CloudTrail logs to verify that there is no EC2 API activity." },
                        { letter: "B", text: "Create an Amazon CloudWatch alarm to stop the EC2 instances when the average CPU utilization is lower than 5% for a 30-minute period." },
                        { letter: "C", text: "Create an Amazon CloudWatch metric to stop the EC2 instances when the VolumeReadBytes metric is lower than 500." },
                        { letter: "D", text: "Use AWS Config to invoke a Lambda function to stop the instances based on resource configuration changes." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "This is a classic use case for Amazon CloudWatch Alarms. CloudWatch natively monitors metrics like CPUUtilization for EC2 instances. You can create an alarm that triggers when a metric crosses a defined threshold for a specified duration. Crucially, CloudWatch Alarms can be configured to take direct actions, including stopping, terminating, or rebooting an EC2 instance. This provides a simple, serverless, and operationally efficient way to meet the requirement.",
                    wrongExplanation: "A: AWS CloudTrail is a service that logs API calls made to your account. It is used for auditing, governance, and compliance, not for monitoring real-time performance metrics like CPU utilization.\n\n**C:** VolumeReadBytes is a metric for disk I/O on an EBS volume. An instance could be idle from a CPU perspective but still have background disk activity, or vice versa. CPU utilization is the metric specified in the requirement and is a much better indicator of whether the application is actively being used.\n\n**D:** AWS Config is a service that tracks changes to your resource configurations. It is used for compliance and configuration management. It does not monitor performance metrics and would not be triggered by low CPU usage."
                },
                {
                    number: 10,
                    title: "RDS Connection Pooling (Based on #328)",
                    scenario: "A company's application using an RDS for MySQL Multi-AZ instance is frequently reporting \"too many connections\" errors. A SysOps administrator needs to resolve this with minimal code changes and in the most cost-effective way.",
                    questionText: "Which solution will meet these requirements MOST cost-effectively?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Modify the RDS for MySQL DB instance to a larger instance size." },
                        { letter: "B", text: "Modify the RDS for MySQL DB instance to Amazon DynamoDB." },
                        { letter: "C", text: "Configure RDS Proxy. Modify the application configuration file to use the RDS Proxy endpoint." },
                        { letter: "D", text: "Modify the RDS for MySQL DB instance to a memory optimized DB instance." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "The \"too many connections\" error indicates that the application is opening and closing connections inefficiently or holding too many connections open, exhausting the database's limit. RDS Proxy is a fully managed, highly available database proxy that is specifically designed to solve this problem. It sits between the application and the database, pooling and sharing database connections. This improves application scalability and resilience by making it more efficient with connection management. The only change required is updating the application's connection endpoint to point to the proxy, which meets the \"minimal code changes\" requirement.",
                    wrongExplanation: "A & D: Scaling the instance up to a larger or memory-optimized size might increase the maximum number of connections, but it's a costly solution that doesn't fix the root cause of inefficient connection management. The application could still exhaust the new, higher limit. RDS Proxy is more cost-effective and addresses the actual problem.\n\n**B:** Migrating from a relational database (MySQL) to a NoSQL database (DynamoDB) is a massive undertaking that would require a complete application rewrite. This violates the \"minimal code changes\" requirement."
                },
                {
                    number: 11,
                    title: "Lambda Internet and VPC Access (Based on #327)",
                    scenario: "A Lambda function, which currently runs outside a VPC and accesses the internet, is being modified. It now needs to store data in an RDS database located in a private subnet of a VPC. The function must maintain its ability to access the internet.",
                    questionText: "Which solution meets these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create a new Lambda function with VPC access and an Elastic IP address." },
                        { letter: "B", text: "Create a new Lambda function with VPC access and two public IP addresses." },
                        { letter: "C", text: "Reconfigure the Lambda function for VPC access. Add NAT gateways to the public subnets. Add route table entries in the private subnets to route through the NAT gateways. Attach the function to the private subnets." },
                        { letter: "D", text: "Reconfigure the Lambda function for VPC access. Attach the function to the private subnets. Add route table entries in the private subnets to route through the internet gateway." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "To access a resource inside a VPC (like an RDS instance in a private subnet), the Lambda function must be configured for VPC access and placed in a subnet within that VPC. Placing it in the private subnet allows it to communicate directly with the RDS instance. However, resources in a private subnet cannot access the internet directly. The standard AWS architecture to grant internet access to resources in a private subnet is to use a NAT Gateway. The NAT Gateway resides in a public subnet and has a route to the Internet Gateway. The private subnet's route table is then configured to send all internet-bound traffic (0.0.0.0/0) to the NAT Gateway. This setup allows the Lambda function to reach both the private RDS instance and the public internet.",
                    wrongExplanation: "A & B: Lambda functions cannot be assigned public or Elastic IP addresses. Placing the function in a public subnet would not allow it to access the RDS instance in the private subnet without complex peering or other networking setups.\n\n**D:** Resources in a private subnet cannot have a route directly to an Internet Gateway. That is the definition of a public subnet. This configuration would not work."
                },
                {
                    number: 12,
                    title: "Centralized Multi-Account Alerting (Based on #326)",
                    scenario: "A company uses AWS Organizations and needs a centralized solution to create standard CloudWatch alarms in all accounts and send alerts to a central logging account when a metric crosses a threshold.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Deploy an AWS CloudFormation stack set to the accounts in the organization. Use a template that creates the required CloudWatch alarms and references an SNS topic in the logging account." },
                        { letter: "B", text: "Deploy an AWS CloudFormation stack in each account." },
                        { letter: "C", text: "Deploy an AWS Lambda function on a cron job in each account." },
                        { letter: "D", text: "Deploy an AWS CloudFormation change set to the organization." }
                    ],
                    correctAnswers: ["A"],
                    explanation: "AWS CloudFormation StackSets are designed for this exact use case. A StackSet allows you to create, update, or delete stacks across multiple accounts and regions with a single operation. You can define a template for your standard CloudWatch alarms and then deploy this template as a StackSet to all accounts within a specific OU or the entire organization. The template can be configured to send notifications to a centralized SNS topic in the logging account (provided the necessary cross-account permissions are set up). This is the most scalable and operationally efficient solution.",
                    wrongExplanation: "B & C: Deploying resources manually or with individual scripts in each account is the opposite of a centralized, operationally efficient solution. It would be difficult to manage and ensure consistency.\n\n**D:** A CloudFormation change set is a preview of the changes a stack update will make. It does not deploy resources across multiple accounts. StackSets are the correct tool for multi-account deployments."
                },
                {
                    number: 13,
                    title: "Alarm for All Unhealthy ALB Targets (Based on #325)",
                    scenario: "A SysOps administrator needs to create a CloudWatch alarm that triggers only when all target instances registered with an Application Load Balancer (ALB) are unhealthy.",
                    questionText: "Which condition should be used with the alarm?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "AWS/ApplicationELB HealthyHostCount <= 0" },
                        { letter: "B", text: "AWS/ApplicationELB UnHealthyHostCount >= 1" },
                        { letter: "C", text: "AWS/EC2 StatusCheckFailed <= 0" },
                        { letter: "D", text: "AWS/EC2 StatusCheckFailed >= 1" }
                    ],
                    correctAnswers: ["A"],
                    explanation: "The requirement is to trigger an alarm when all hosts are unhealthy. The most direct way to measure this is to check the number of healthy hosts. The HealthyHostCount metric for an ALB's target group tracks the number of healthy instances. If all instances are unhealthy, this count will be zero. Therefore, setting an alarm to trigger when HealthyHostCount is less than or equal to 0 (<= 0) precisely matches the condition.",
                    wrongExplanation: "B: The UnHealthyHostCount >= 1 condition would trigger an alarm if even a single instance becomes unhealthy. The requirement is for all instances to be unhealthy.\n\n**C & D:** These are EC2 metrics, not ALB metrics. While related, they monitor the health of an individual instance from the EC2 perspective, not from the perspective of the ALB's health checks. The ALB could mark an instance as unhealthy (e.g., the application is not responding) even if the EC2 status checks are passing. The ALB metrics are the correct ones to use for this scenario."
                },
                {
                    number: 14,
                    title: "Scaling a CPU-Heavy Application (Based on #322)",
                    scenario: "A legacy, CPU-intensive application runs on a single t3.large EC2 instance and can only be scaled vertically. The instance is experiencing 90% CPU usage and performance latency.",
                    questionText: "What change should be made to alleviate the performance problem?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Change the Amazon EBS volume to Provisioned IOPS." },
                        { letter: "B", text: "Upgrade to a compute-optimized instance." },
                        { letter: "C", text: "Add additional t2.large instances to the application." },
                        { letter: "D", text: "Purchase Reserved Instances." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "The problem is clearly stated as high CPU usage (90%) causing a performance bottleneck. The application is \"CPU-heavy.\" The most direct solution is to provide the application with more CPU power. Compute-optimized instance families (like the C-family, e.g., c5.large) are specifically designed for compute-bound applications that require high-performance processors. Upgrading to an instance from this family will directly address the CPU bottleneck. This aligns with the \"vertical scaling\" constraint.",
                    wrongExplanation: "A: Changing the EBS volume type addresses disk I/O performance. Since the bottleneck is CPU, this change would likely have no impact on the problem.\n\n**C:** The question explicitly states the application can only be scaled vertically (increasing the size/power of a single instance), not horizontally (adding more instances). This option violates that constraint.\n\n**D:** Purchasing Reserved Instances is a billing construct that provides a discount in exchange for a commitment to use EC2. It does not change the performance or specifications of the instance itself."
                },
                {
                    number: 15,
                    title: "Private EC2 Instance Internet Connectivity (Based on #318)",
                    scenario: "A SysOps administrator launches an EC2 instance in a private subnet. When trying to run a curl command to an external website (https://www.example.com), the connection fails.",
                    questionText: "What should the SysOps administrator do to resolve this issue?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Ensure that there is an outbound security group for port 443 to 0.0.0.0/0." },
                        { letter: "B", text: "Ensure that there is an inbound security group for port 443 from 0.0.0.0/0." },
                        { letter: "C", text: "Ensure that there is an outbound network ACL for ephemeral ports 1024-65535 to 0.0.0.0/0." },
                        { letter: "D", text: "Ensure that there is an outbound network ACL for port 80 to 0.0.0.0/0." }
                    ],
                    correctAnswers: ["A"],
                    explanation: "Security Groups (Stateful): A curl to https://www.example.com is an outbound request on port 443 (HTTPS). By default, a security group's outbound rules allow all traffic. However, if this default has been removed, you would need to explicitly add an outbound rule allowing traffic on TCP port 443 to the internet (0.0.0.0/0). Because security groups are stateful, this single outbound rule is sufficient; the return traffic is automatically allowed.",
                    wrongExplanation: "B: An inbound security group rule is for traffic coming to the instance. This is an outbound connection.\n\n**C:** The outbound request is to destination port 443, not to an ephemeral port. The return traffic comes back to an ephemeral port on the instance, which would require an inbound NACL rule.\n\n**D:** The request is to https, which uses port 443, not http which uses port 80."
                },
                {
                    number: 16,
                    title: "Automated Instance Reboot on High CPU (Based on #317)",
                    scenario: "A legacy application causes errors when CPU utilization on its EC2 instance exceeds 80%. A short-term solution is needed to automatically reboot the instance when this happens. The solution should have the LEAST operational overhead.",
                    questionText: "Which solution meets these requirements with the LEAST operational overhead?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Write a script that runs as a cron job to monitor and reboot the instance." },
                        { letter: "B", text: "Add an Amazon CloudWatch alarm for CPU utilization and configure the alarm action to reboot the EC2 instances." },
                        { letter: "C", text: "Create an Amazon EventBridge rule to invoke a Lambda function to restart the instances." },
                        { letter: "D", text: "Add a CloudWatch alarm and configure an AWS Systems Manager Automation runbook to reboot the instances." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "This solution is the most direct and has the least operational overhead. CloudWatch Alarms can be configured to take several built-in actions directly, one of which is to reboot an EC2 instance. This requires no scripting, no Lambda functions, and no Systems Manager runbooks. You simply create the alarm, set the metric (CPUUtilization > 80%), and select \"Reboot this instance\" from the dropdown list of actions. It's a native, point-and-click (or single API call) solution.",
                    wrongExplanation: "A: Writing and maintaining a custom script and cron job on the instance itself introduces significant operational overhead compared to using a managed AWS service.\n\n**C & D:** While both of these solutions would work, they are more complex and have more operational overhead than the direct CloudWatch alarm action. They involve configuring multiple services (EventBridge + Lambda, or CloudWatch + SSM) to accomplish something CloudWatch can do on its own. Therefore, they do not have the least overhead."
                },
                {
                    number: 17,
                    title: "Enforcing Standardized EC2 Configurations (Based on #311)",
                    scenario: "A company wants to ensure that all business units can only provision EC2 instances using pre-approved, standardized configurations.",
                    questionText: "What should a SysOps administrator do to implement this requirement?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an EC2 instance launch configuration." },
                        { letter: "B", text: "Develop an IAM policy that limits the business units to provision EC2 instances only." },
                        { letter: "C", text: "Publish a product and launch constraint role for EC2 instances by using AWS Service Catalog. Allow the business units to perform actions in AWS Service Catalog only." },
                        { letter: "D", text: "Share an AWS CloudFormation template with the business units." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "AWS Service Catalog is the service designed for creating and managing catalogs of IT services that are approved for use on AWS. An administrator can define a \"product\" (e.g., a CloudFormation template for an approved EC2 instance configuration) and add it to a portfolio. They can then grant business units access to this portfolio. Users can then launch these pre-approved products without needing underlying permissions to the services themselves, thanks to launch constraint roles. This provides governance, control, and standardization, perfectly matching the requirement.",
                    wrongExplanation: "A: Launch configurations are an older component of Auto Scaling groups and don't provide a broad governance mechanism for all instance launches.\n\n**B & D:** Simply providing an IAM policy or a CloudFormation template doesn't enforce the use of the approved configuration. Users with EC2 permissions could still launch any instance type they want, or modify the shared template. Service Catalog provides the necessary enforcement and governance layer."
                },
                {
                    number: 18,
                    title: "Notification on EC2 Instance Launch (Based on #309)",
                    scenario: "A company's architecture team requires immediate email notification whenever a new EC2 instance is launched in the production account.",
                    questionText: "What should a SysOps administrator do to meet this requirement?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Use a user data script to send an email." },
                        { letter: "B", text: "Create an Amazon SNS topic with an email subscription. Create an Amazon EventBridge rule that reacts to EC2 instance launches and targets the SNS topic." },
                        { letter: "C", text: "Use an Amazon SQS queue with an email subscription." },
                        { letter: "D", text: "Use AWS Systems Manager to publish events to an SNS topic, which is polled by a Lambda function." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "This is the standard, event-driven, serverless pattern for this type of notification. Amazon EventBridge (formerly CloudWatch Events) can capture events happening in your AWS account, such as an EC2 instance changing to the \"running\" state. You create a rule that matches this specific event. The rule's target can be an Amazon SNS topic. You create an SNS topic and subscribe the architecture team's email address to it. When a new instance is launched, EventBridge catches the event and publishes a message to the SNS topic, which then sends an email to all subscribers. This is efficient, scalable, and decoupled.",
                    wrongExplanation: "A: Relying on a user data script is unreliable. It might fail, or someone might launch an instance without the script. It's not a centralized or guaranteed solution.\n\n**C:** Amazon SQS (Simple Queue Service) is a message queue; it cannot send emails directly. You would need another service (like Lambda) to poll the queue and then send the email, making it more complex than option B.\n\n**D:** This is an overly complex and convoluted architecture. EventBridge is the primary service for reacting to AWS API events, and SNS can send emails directly. There is no need for Systems Manager or a polling Lambda function."
                },
                {
                    number: 19,
                    title: "Granular Cost and Usage Dashboards (Based on #307)",
                    scenario: "A company's finance team needs detailed dashboards to track AWS cost changes across the entire organization, with granularity down to the hour.",
                    questionText: "What is the MOST operationally efficient way to meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Generate Amazon CloudWatch dashboards by using CloudWatch insights and AWS Cost Explorer data." },
                        { letter: "B", text: "Generate an AWS Cost and Usage Report (CUR). Store it in S3. Use Amazon Athena to query the data and Amazon QuickSight to build dashboards." },
                        { letter: "C", text: "Create a Lambda function that runs daily to pull data from Cost Explorer." },
                        { letter: "D", text: "Create an IAM user for the finance team with access to Cost Explorer." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "This describes the standard, recommended AWS architecture for detailed cost analysis, often called the Cloud Intelligence Dashboards framework. The AWS Cost and Usage Report (CUR) is the most comprehensive source of cost and usage data and can be configured for hourly granularity. By delivering the CUR to an S3 bucket, you create a data lake of your billing information. Amazon Athena can then be used to run complex SQL queries directly on these files in S3. Finally, Amazon QuickSight can connect to Athena as a data source to build powerful, interactive, and shareable dashboards for the finance team. This entire pipeline is scalable and highly efficient once set up.",
                    wrongExplanation: "A: Cost Explorer data cannot be directly integrated into CloudWatch dashboards in this manner, and CloudWatch Logs Insights is for analyzing application/system logs, not billing data.\n\n**C:** Cost Explorer's API does not provide the same level of detail as the CUR, and running a daily Lambda function does not meet the hourly granularity requirement.\n\n**D:** Simply giving access to the Cost Explorer console is not sufficient. Cost Explorer is great for high-level analysis, but it doesn't offer the deep, customizable query capabilities and dashboarding features of the Athena + QuickSight solution needed for detailed financial tracking."
                },
                {
                    number: 20,
                    title: "High-Performance Temporary Cache (Based on #306)",
                    scenario: "A workload on an EC2 instance needs a temporary cache for frequently changing data. The highest possible performance is required, and the data does not need to be retained if the instance restarts.",
                    questionText: "Which storage option will provide the HIGHEST performance for the cache?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume" },
                        { letter: "B", text: "Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume" },
                        { letter: "C", text: "Throughput Optimized HDD (st1) Amazon Elastic Block Store (Amazon EBS) volume" },
                        { letter: "D", text: "EC2 instance store" }
                    ],
                    correctAnswers: ["D"],
                    explanation: "EC2 instance store (also known as ephemeral storage) provides block-level storage that is physically attached to the host computer running the EC2 instance. Because it is directly attached, it offers the lowest latency and highest I/O performance possible, making it ideal for high-performance caches, scratch disks, or buffers. The key tradeoff, which is acceptable in this scenario, is that the data is non-persistent; it is lost if the instance is stopped, hibernated, or terminated.",
                    wrongExplanation: "A, B, C: These are all types of Amazon EBS volumes. EBS provides persistent, network-attached storage. While high-performance options like io2 exist, they will always have slightly higher latency than a physically attached instance store because the data has to travel over the AWS network to reach the volume. Since the highest performance is the goal and persistence is not needed, instance store is the superior choice."
                },
                {
                    number: 21,
                    title: "Troubleshooting CloudWatch Agent Permissions (Based on #305)",
                    scenario: "A SysOps administrator is troubleshooting an Amazon Linux 2 EC2 instance where the CloudWatch agent is running and correctly configured, but no logs are being published to CloudWatch Logs.",
                    questionText: "What should the SysOps administrator do to resolve the issue?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Configure the AWS CLI and use a cron job to push logs." },
                        { letter: "B", text: "Inspect the retention period of the log group." },
                        { letter: "C", text: "Set up an Amazon Kinesis data stream." },
                        { letter: "D", text: "Ensure that the IAM role that is attached to the EC2 instance has the necessary permissions for CloudWatch Logs." }
                    ],
                    correctAnswers: ["D"],
                    explanation: "For the CloudWatch agent on an EC2 instance to send logs to the CloudWatch Logs service, it needs permission to make API calls to that service. These permissions are granted via an IAM role attached to the instance. If the agent is running and configured correctly, the most common cause of failure is a missing or incorrect IAM role. The role must have a policy that allows actions such as logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents, and logs:DescribeLogStreams.",
                    wrongExplanation: "A: This is a cumbersome workaround that bypasses the agent. The goal is to fix the agent, not replace its functionality with a custom script.\n\n**B:** The log group retention period determines how long logs are kept after they arrive. It has no effect on whether the agent can send the logs in the first place. If no logs are arriving, a log group might not even exist yet.\n\n**C:** Kinesis is a service for streaming data at scale. It's not required for the basic functionality of the CloudWatch agent and would add unnecessary complexity. The agent is designed to send logs directly to CloudWatch Logs."
                },
                {
                    number: 22,
                    title: "Simultaneous File Access for Windows Servers",
                    scenario: "A company hosts a Windows-based file server on a fleet of Amazon EC2 instances spread across multiple Availability Zones. The application servers are currently unable to access files simultaneously from this fleet.",
                    questionText: "Which solution will allow simultaneous file access from multiple application servers in the MOST operationally efficient way?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Copy the files to the EFS file system. Connect the EFS file system to mount points on the application servers." },
                        { letter: "B", text: "Create an Amazon FSx for Windows File Server Multi-AZ file system. Copy the files to the Amazon FSx file system. Adjust the connections from the application servers to use the share that the Amazon FSx file system exposes." },
                        { letter: "C", text: "Create an Amazon Elastic Block Store (Amazon EBS) volume that has EBS Multi-Attach enabled. Create an Auto Scaling group for the Windows file server. Use a script in the file server's user data to attach the Shared FileAccess tag to the EBS volume during launch." },
                        { letter: "D", text: "Create two Amazon FSx for Windows File Server file systems. Configure Distributed File System (DFS) replication between the file systems. Copy the files to the Amazon FSx file systems. Adjust the connections from the application servers to use the shares that the Amazon FSx file systems expose." }
                    ],
                    correctAnswers: ["B"],
                    explanation: "This question requires a shared file storage solution that is compatible with Windows and highly available across multiple Availability Zones.\nWhy B is correct: Amazon FSx for Windows File Server is a fully managed service that provides shared file storage built on Windows Server. It natively supports the SMB protocol, Windows ACLs, and Multi-AZ deployments. This makes it the perfect fit for providing highly available, shared file access to Windows-based EC2 instances without requiring complex manual configuration.",
                    wrongExplanation: "A: Amazon EFS is designed for Linux-based workloads and uses the NFS protocol. While it can be accessed from Windows instances, it's not the native or most efficient solution for a Windows environment.\n\n**C:** EBS Multi-Attach allows an EBS volume to be attached to multiple Nitro-based instances, but only within the same Availability Zone. This does not meet the requirement of being accessible across multiple AZs.\n\n**D:** While using two FSx file systems with DFS replication would work, it is not the most operationally efficient solution. A single Multi-AZ FSx file system handles the replication and failover automatically, reducing management overhead compared to setting up and managing DFS replication manually."
                },
                {
                    number: 23,
                    title: "Automated and Protected EBS Backups",
                    scenario: "A company uses AWS Organizations to manage a multi-account environment. They need to automate the creation of daily incremental backups for any Amazon EBS volume tagged with Lifecycle: Production. A key requirement is to prevent users from deleting these production snapshots using their standard EC2 permissions.",
                    questionText: "What should a SysOps administrator do to meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create a daily snapshot of all EBS volumes by using Amazon Data Lifecycle Manager. Specify Lifecycle as the tag key. Specify Production as the tag value." },
                        { letter: "B", text: "Associate a service control policy (SCP) with the account to deny users the ability to delete EBS snapshots. Create an Amazon EventBridge rule with a 24-hour cron schedule. Configure EBS Create Snapshot as the target. Target all EBS volumes with the specified tags." },
                        { letter: "C", text: "Create a daily snapshot of all EBS volumes by using AWS Backup. Specify Lifecycle as the tag key. Specify Production as the tag value." },
                        { letter: "D", text: "Create a daily Amazon Machine Image (AMI) of every production EC2 instance within the AWS account by using Amazon Data Lifecycle Manager." }
                    ],
                    correctAnswers: ["C"],
                    explanation: "The core requirements are automated, tag-based, incremental backups and protection against deletion.\nWhy C is correct: AWS Backup is a centralized backup service that simplifies the management of backups across AWS services. It can create tag-based backup plans to automatically take daily incremental snapshots. Crucially, AWS Backup has a feature called Backup Vault Lock, which can enforce write-once, read-many (WORM) policies. This prevents anyone, including administrators, from deleting the backups before the retention period expires, directly meeting the deletion prevention requirement.",
                    wrongExplanation: "A: Amazon Data Lifecycle Manager (DLM) can automate snapshot creation based on tags, but it does not have a built-in, robust feature like Vault Lock to prevent users with ec2:DeleteSnapshot permissions from deleting the snapshots it creates.\n\n**B:** This solution is overly complex and has flaws. While an SCP can deny ec2:DeleteSnapshot, it would be a blanket denial and could interfere with legitimate operations. Using EventBridge to trigger snapshots is less efficient than using a dedicated backup service. AWS Backup is the more integrated and appropriate tool.\n\n**D:** The requirement is to back up EBS volumes, not entire EC2 instances. Creating an AMI is unnecessary and would back up more data than required."
                },
                {
                    number: 24,
                    title: "Static IP for Outbound API Calls",
                    scenario: "An application runs on hundreds of Amazon EC2 instances distributed across three Availability Zones. This application needs to make calls to a third-party API over the public internet. The third-party provider requires a static list of IP addresses to add to their allow list.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Add a NAT gateway in the public subnet of each Availability Zone. Make the NAT gateway the default route of all private subnets in those Availability Zones." },
                        { letter: "B", text: "Allocate one Elastic IP address in each Availability Zone. Associate the Elastic IP address with all the instances in the Availability Zone." },
                        { letter: "C", text: "Place the instances behind a Network Load Balancer (NLB). Send the traffic to the internet through the private IP address of the NLB." },
                        { letter: "D", text: "Update the main route table to send the traffic to the internet through an Elastic IP address that is assigned to each instance." }
                    ],
                    correctAnswers: ["A"],
                    explanation: "The goal is to provide a small, fixed number of public IP addresses for a large, dynamic fleet of EC2 instances making outbound connections.\nWhy A is correct: This is the classic and recommended architecture for this scenario. By placing a NAT Gateway in each Availability Zone's public subnet and routing outbound traffic from the private subnets through it, all instances in that AZ will appear to originate from the single, static Elastic IP address associated with that NAT Gateway. This provides a stable, highly available solution with only three IP addresses to give to the third party.",
                    wrongExplanation: "B: An Elastic IP address can only be associated with one EC2 instance at a time. It's not possible to associate a single EIP with \"all the instances\" in an AZ.\n\n**C:** Load balancers (both NLB and ALB) are designed for managing inbound traffic to your instances, not for routing outbound traffic from them.\n\n**D:** Assigning an Elastic IP address to each of the \"hundreds\" of instances is not operationally efficient, would be costly, and would result in a very long list of IP addresses to provide to the third party, which defeats the purpose of simplifying the allow list."
                },
                {
                    number: 25,
                    title: "Serving S3 Content Privately",
                    scenario: "A SysOps administrator is setting up an Amazon S3 bucket to host a static web application. The files have been copied to the bucket. A strict company policy dictates that all S3 buckets must remain private and not be publicly accessible.",
                    questionText: "What should the SysOps administrator do to meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an Amazon CloudFront distribution. Configure the S3 bucket as an origin with an origin access identity (OAI). Give the OAI the s3:GetObject permission in the S3 bucket policy." },
                        { letter: "B", text: "Configure static website hosting in the S3 bucket. Use Amazon Route 53 to create a DNS CNAME to point to the S3 website endpoint." },
                        { letter: "C", text: "Create an Application Load Balancer (ALB). Change the protocol to HTTPS in the ALB listener configuration. Forward the traffic to the S3 bucket." },
                        { letter: "D", text: "Create an accelerator in AWS Global Accelerator. Set up a listener configuration for port 443. Set the endpoint type to forward the traffic to the S3 bucket." }
                    ],
                    correctAnswers: ["A"],
                    explanation: "The challenge is to serve content to the public from an S3 bucket that itself must remain private.\nWhy A is correct: This is the standard and most secure method for this use case. Amazon CloudFront can act as the public-facing entry point. An Origin Access Identity (OAI) is a special CloudFront user that you can grant permissions to access your private S3 bucket. You can then configure the S3 bucket policy to only allow access from this OAI, keeping the bucket private from all other public access. Users access the content via CloudFront, which securely fetches it from the private bucket.",
                    wrongExplanation: "B: Configuring static website hosting on an S3 bucket requires the bucket and its objects to be made public, which directly violates the company policy.\n\n**C and D:** While an ALB or Global Accelerator can route traffic, they are not the primary or most direct services for serving content from S3 while keeping the bucket private. CloudFront with OAI is the purpose-built solution for this exact scenario."
                },
                {
                    number: 26,
                    title: "Secure SQS Access from EC2",
                    scenario: "An application running on an Amazon EC2 instance needs to interact with Amazon SQS queues. Specifically, it must be able to read, write, and delete messages.",
                    questionText: "Which solution will meet these requirements in the MOST secure manner?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Create an IAM user with an IAM policy that allows the sqs:SendMessage, sqs:ReceiveMessage, and sqs:DeleteMessage permission. Embed the IAM user's credentials in the application's configuration." },
                        { letter: "B", text: "Create an IAM user with an IAM policy that allows the sqs:SendMessage, sqs:ReceiveMessage, and sqs:DeleteMessage permission. Export the IAM user's access key and secret access key as environment variables on the EC2 instance." },
                        { letter: "C", text: "Create and associate an IAM role that allows EC2 instances to call AWS services. Attach an IAM policy to the role that allows sqs:* permissions to the appropriate queues." },
                        { letter: "D", text: "Create and associate an IAM role that allows EC2 instances to call AWS services. Attach an IAM policy to the role that allows the sqs:SendMessage, sqs:ReceiveMessage, and sqs:DeleteMessage permission to the appropriate queues." }
                    ],
                    correctAnswers: ["D"],
                    explanation: "This question tests the best practices for granting permissions to AWS resources, emphasizing security.\nWhy D is correct: This option follows two key security best practices. First, it uses an IAM Role associated with the EC2 instance. This is superior to using IAM user credentials because the credentials are automatically rotated and managed by AWS, eliminating the risk of long-lived keys being exposed. Second, it adheres to the Principle of Least Privilege by granting only the specific permissions required (SendMessage, ReceiveMessage, DeleteMessage) rather than a broad wildcard.",
                    wrongExplanation: "A and B: Both of these options involve using long-lived IAM user credentials (access key and secret key) and storing them on the instance. This is a significant security risk. If the instance is ever compromised, the attacker gains access to these keys. IAM roles are the recommended alternative.\n\n**C:** While using an IAM role is correct, this option violates the principle of least privilege by using a wildcard (sqs:*). This grants the application far more permissions than it needs (e.g., permissions to create or delete queues), increasing the potential impact if the application's credentials are ever compromised."
                },
                {
                    number: 27,
                    title: "CloudFront Caching Issue",
                    scenario: "A SysOps administrator has configured an Amazon CloudFront distribution with an Application Load Balancer (ALB) as the origin to reduce load on the web servers. After a week, monitoring shows that requests are still being served directly by the ALB, and there's no change in the web server load.",
                    questionText: "What are possible causes for this problem? (Choose two.)",
                    isMultiChoice: true,
                    options: [
                        { letter: "A", text: "CloudFront does not have the ALB configured as the origin access identity." },
                        { letter: "B", text: "The DNS is still pointing to the ALB instead of the CloudFront distribution." },
                        { letter: "C", text: "The ALB security group is not permitting inbound traffic from CloudFront." },
                        { letter: "D", text: "The default, minimum, and maximum Time to Live (TTL) are set to 0 seconds on the CloudFront distribution." },
                        { letter: "E", text: "The target groups associated with the ALB are configured for sticky sessions." }
                    ],
                    correctAnswers: ["B", "D"],
                    explanation: "The problem is that CloudFront is not caching or serving the traffic as expected.\nWhy B is correct: If the public DNS record (e.g., www.example.com) still points directly to the ALB's DNS name, users will bypass CloudFront entirely. For CloudFront to serve traffic, the public DNS record must be updated to point to the CloudFront distribution's domain name (e.g., d12345.cloudfront.net).\nWhy D is correct: The Time to Live (TTL) setting in CloudFront's cache behavior tells CloudFront how long to cache an object at the edge location before checking with the origin (the ALB) again. If the TTL is set to 0, CloudFront will forward every single request to the ALB to check for an updated version. This effectively disables caching and would result in no load reduction on the origin servers.",
                    wrongExplanation: "A: Origin Access Identity (OAI) is used to restrict access to S3 bucket origins, not ALB origins. For an ALB, you would typically use custom headers and security group rules to restrict access.\n\n**C:** If the ALB security group blocked traffic from CloudFront, users would receive errors (e.g., 502 Bad Gateway) from CloudFront. The problem described is that traffic is still being served, just not from the cache.\n\n**E:** Sticky sessions on the ALB ensure that a user is consistently routed to the same backend EC2 instance. This would not prevent CloudFront from caching content."
                },
                {
                    number: 28,
                    title: "Creating a New RDS Cluster from Backup",
                    scenario: "An Amazon RDS for PostgreSQL DB cluster has automated backups enabled with a 7-day retention period. A SysOps administrator needs to create a new, separate RDS DB cluster using data that is no more than 24 hours old from the original cluster.",
                    questionText: "Which solutions will meet these requirements with the LEAST operational overhead? (Choose two.)",
                    isMultiChoice: true,
                    options: [
                        { letter: "A", text: "Identify the most recent automated snapshot. Restore the snapshot to a new RDS DB cluster." },
                        { letter: "B", text: "Back up the database to Amazon S3 by using native database backup tools. Create a new RDS DB cluster and restore the data to the new RDS DB cluster." },
                        { letter: "C", text: "Create a read replica instance in the original RDS DB cluster. Promote the read replica to a standalone DB cluster." },
                        { letter: "D", text: "Create a new RDS DB cluster. Use AWS Database Migration Service (AWS DMS) to migrate data from the current RDS DB cluster to the newly created RDS DB cluster." },
                        { letter: "E", text: "Use the pg_dump utility to export data from the original RDS DB cluster to an Amazon EC2 instance. Create a new RDS DB cluster. Use the pg_restore utility to import the data from the EC2 instance to the new RDS DB cluster." }
                    ],
                    correctAnswers: ["A", "C"],
                    explanation: "The goal is to create a new, independent cluster from recent data with minimal effort.\nWhy A is correct: RDS automated backups include daily snapshots and transaction logs. This allows for point-in-time recovery. Restoring from the most recent automated snapshot is a simple, built-in RDS feature that can be done with a few clicks or a single API call, representing very low operational overhead.\nWhy C is correct: Creating a read replica provides an asynchronously updated, read-only copy of the database. This replica can be \"promoted\" to become a new, independent, writeable DB cluster at any time. This is also a standard, low-overhead RDS operation. The data on the replica is typically only seconds or minutes behind the primary, easily meeting the \"less than 24 hours old\" requirement.",
                    wrongExplanation: "B and E: Using native database tools like pg_dump involves manual steps: connecting to the database, running the backup, managing the backup file, connecting to the new database, and running the restore. This is significantly more operational overhead than using the built-in RDS features.\n\n**D:** AWS DMS is a powerful service designed for migrating databases, often between different database engines or from on-premises to AWS. Using it to simply clone an existing RDS cluster is overkill and involves more setup and configuration (e.g., creating replication instances, defining tasks) than restoring a snapshot or promoting a replica."
                },
                {
                    number: 29,
                    title: "CloudFormation S3 Bucket Creation Failure",
                    scenario: "A user, authenticated via Active Directory federation, attempts to deploy an AWS CloudFormation template that creates an Amazon S3 bucket. The stack creation fails.",
                    questionText: "Which factors could cause this failure? (Choose two.)",
                    isMultiChoice: true,
                    options: [
                        { letter: "A", text: "The user's IAM policy does not allow the cloudformation:CreateStack action." },
                        { letter: "B", text: "The user's IAM policy does not allow the cloudformation:CreateStackSet action." },
                        { letter: "C", text: "The user's IAM policy does not allow the s3:CreateBucket action." },
                        { letter: "D", text: "The user's IAM policy explicitly denies the s3:ListBucket action." },
                        { letter: "E", text: "The user's IAM policy explicitly denies the s3:PutObject action." }
                    ],
                    correctAnswers: ["A", "C"],
                    explanation: "When a user deploys a CloudFormation stack, two sets of permissions are involved: the user's permission to interact with CloudFormation, and CloudFormation's permission to create the resources in the template.\nWhy A is correct: To initiate the stack creation process, the user themselves must have the cloudformation:CreateStack permission. If this is missing, the request to create the stack will be denied immediately.\nWhy C is correct: CloudFormation acts on behalf of the user who initiated the stack creation (unless a service role is specified). Therefore, the user's IAM credentials must also include the permissions needed to create the resources defined in the template. In this case, to create an S3 bucket, the user needs the s3:CreateBucket permission. If this is missing, CloudFormation will fail when it attempts to provision the bucket.",
                    wrongExplanation: "B: CreateStackSet is for deploying stacks across multiple accounts and regions. The scenario describes deploying a single stack, which uses the CreateStack action.\n\n**D and E:** The s3:ListBucket and s3:PutObject actions are for interacting with an existing bucket (listing its contents or adding objects). They are not required for the initial creation of the bucket itself. The stack would fail specifically at the CreateBucket step."
                },
                {
                    number: 30,
                    title: "S3 Encryption with Audit Trail",
                    scenario: "A company is building a financial application that stores sensitive data in Amazon S3. The data must be encrypted at rest. The company does not want to manage its own encryption keys but requires an audit trail of when and by whom the keys are used.",
                    questionText: "Which solution will meet these requirements?",
                    isMultiChoice: false,
                    options: [
                        { letter: "A", text: "Use client-side encryption with client-provided keys. Upload the encrypted user data to Amazon S3." },
                        { letter: "B", text: "Use server-side encryption with S3 managed encryption keys (SSE-S3) to encrypt the user data on Amazon S3." },
                        { letter: "C", text: "Use server-side encryption with customer-provided encryption keys (SSE-C) to encrypt the user data on Amazon S3." },
                        { letter: "D", text: "Use server-side encryption with AWS KMS managed encryption keys (SSE-KMS) to encrypt the user data on Amazon S3." }
                    ],
                    correctAnswers: ["D"],
                    explanation: "The key requirements are AWS-managed keys and a detailed audit trail of key usage.\nWhy D is correct: Server-Side Encryption with AWS Key Management Service (SSE-KMS) meets both requirements perfectly. AWS manages the lifecycle of the keys (so the company doesn't have to), but the company retains control over the key's policy. Crucially, every time this KMS key is used to encrypt or decrypt an S3 object, the action is logged in AWS CloudTrail. This provides the detailed audit trail of key usage (who, what, when) that the company requires.",
                    wrongExplanation: "A and C: Both of these options involve the company providing and managing their own encryption keys, which violates the requirement of not wanting to manage their own keys.\n\n**B:** Server-Side Encryption with S3-Managed Keys (SSE-S3) is the simplest form of encryption. AWS fully manages the keys and the encryption process. However, it does not provide a separate, detailed CloudTrail audit log for the usage of the data keys. The control and auditability are much lower than with SSE-KMS."
                },
                 {
                    number: 52,
                    title: "Web Server Accessibility Troubleshooting",
                    scenario: "A SysOps administrator has set up a new Amazon EC2 instance as a web server in a public subnet. The instance uses HTTP port 80 and HTTPS port 443. The SysOps administrator has confirmed internet connectivity by downloading operating system updates and software from public repositories. However, the SysOps administrator cannot access the instance from a web browser on the internet.",
                    questionText: "Which combination of steps should the SysOps administrator take to troubleshoot this issue? (Choose three.)",
                    isMultiChoice: true,
                    options: [
                        { letter: "A", text: "Ensure that the inbound rules of the instance's security group allow traffic on ports 80 and 443." },
                        { letter: "B", text: "Ensure that the outbound rules of the instance's security group allow traffic on ports 80 and 443." },
                        { letter: "C", text: "Ensure that ephemeral ports 1024-65535 are allowed in the inbound rules of the network ACL that is associated with the instance's subnet." },
                        { letter: "D", text: "Ensure that ephemeral ports 1024-65535 are allowed in the outbound rules of the network ACL that is associated with the instance's subnet." },
                        { letter: "E", text: "Ensure that the filtering rules for any firewalls that are running on the instance allow inbound traffic on ports 80 and 443." },
                        { letter: "F", text: "Ensure that AWS WAF is turned on for the instance and is blocking web traffic." }
                    ],
                    correctAnswers: ["A", "D", "E"],
                    explanation: "Why A is correct: Security groups act as virtual firewalls for EC2 instances. If the inbound rules for ports 80 and 443 are not open, external web traffic will be blocked from reaching the instance.\n\nWhy D is correct: Network ACLs are stateless, meaning both inbound and outbound rules must explicitly allow traffic. When a client initiates a connection to a web server (e.g., on port 80 or 443), the server's response traffic uses ephemeral ports for the return communication. Therefore, the outbound rules of the network ACL must allow traffic on these ephemeral ports for the web server to send responses back to the client.\n\nWhy E is correct: An EC2 instance, especially a Microsoft Windows Server, can have its own operating system-level firewall (like Windows Firewall) configured. If this internal firewall is blocking inbound traffic on ports 80 and 443, web browsers will be unable to access the application, even if AWS security groups and Network ACLs are correctly configured.",
                    wrongExplanation: "B: The question states that downloading OS updates worked, which implies outbound traffic on common ports (like 80 and 443 for updates) is already functioning. The problem is with inbound web access.\n\n**C:** Ephemeral ports are primarily needed for the outbound return traffic from the web server to the user's client, not for inbound traffic to the web server itself on ports 80 and 443.\n\n**F:** AWS WAF (Web Application Firewall) is not a default component in a basic EC2 web server setup and is generally used for more advanced web application protection. Assuming it's the cause of a basic access denied error in an initial setup is unlikely and not a primary troubleshooting step."
                }
                // ... More questions would be added here in the same structured format
            ];

            // --- QUIZ LOGIC ---
            function initializeQuiz() {
                // Add userAnswers property to each question for state management
                questions.forEach(q => q.userAnswers = []);
                currentQuestionIndex = 0;
                updateScore();
                populateQuestionSelect();
                displayQuestion(currentQuestionIndex);
            }

            function displayQuestion(index) {
                currentQuestionIndex = index;
                const question = questions[index];
                quizContainer.innerHTML = '';

                if (!question) {
                    quizContainer.innerHTML = `<p class="text-center text-red-500">Error: Question not found.</p>`;
                    return;
                }

                // Sanitize and format the question text to be displayed
                const formattedQuestionText = question.questionText.replace(/\(Choose (two|three)\.\)/, '<span class="block text-sm text-gray-500 font-normal mt-1">($&.toUpperCase())</span>');

                const questionEl = document.createElement('div');
                questionEl.innerHTML = `
                    <div class="mb-4">
                        <p class="text-sm font-semibold text-gray-500">Question ${question.number} / ${questions.length}</p>
                        <h2 class="text-xl font-bold mt-1">${question.title}</h2>
                    </div>
                    ${question.scenario ? `<div class="mb-4 p-4 bg-gray-50 rounded-md border border-gray-200"><h3 class="font-semibold mb-2">Scenario</h3><p class="text-gray-600 whitespace-pre-wrap">${question.scenario}</p></div>` : ''}
                    <div class="mb-6"><p class="font-semibold whitespace-pre-wrap">${formattedQuestionText}</p></div>
                    <div id="options-container" class="grid grid-cols-1 gap-3"></div>
                    <div id="explanation-container" class="mt-6"></div>
                `;
                quizContainer.appendChild(questionEl);

                const optionsContainer = document.getElementById('options-container');
                const isAnswered = question.userAnswers.length > 0;

                question.options.forEach(option => {
                    const optionWrapper = document.createElement('div');
                    optionWrapper.classList.add('flex', 'items-center');

                    const inputType = question.isMultiChoice ? 'checkbox' : 'radio';
                    const input = document.createElement('input');
                    input.type = inputType;
                    input.id = `q${question.number}_${option.letter}`;
                    input.name = `q${question.number}_options`;
                    input.value = option.letter;
                    input.classList.add('hidden'); // Hide the actual radio/checkbox
                    
                    if (question.userAnswers.includes(option.letter)) {
                        input.checked = true;
                    }

                    const label = document.createElement('label');
                    label.htmlFor = `q${question.number}_${option.letter}`;
                    label.classList.add('option-btn', 'w-full', 'text-left', 'p-4', 'border-2', 'rounded-lg', 'cursor-pointer');
                    label.innerHTML = `<span class="font-bold mr-2">${option.letter}.</span> ${option.text}`;

                    if (input.checked) {
                         if (isAnswered) {
                            if (question.correctAnswers.includes(option.letter)) {
                                label.classList.add('correct');
                            } else {
                                label.classList.add('incorrect');
                            }
                        }
                    }

                    label.addEventListener('click', (e) => {
                        e.preventDefault(); // Prevent default label behavior
                        handleAnswerClick(option.letter);
                    });

                    optionWrapper.appendChild(input);
                    optionWrapper.appendChild(label);
                    optionsContainer.appendChild(optionWrapper);
                });

                if (isAnswered) {
                    showExplanationButton();
                }

                updateNavigation();
            }

            function handleAnswerClick(selectedOptionLetter) {
                const question = questions[currentQuestionIndex];

                if (question.isMultiChoice) {
                    const answerIndex = question.userAnswers.indexOf(selectedOptionLetter);
                    if (answerIndex > -1) {
                        question.userAnswers.splice(answerIndex, 1); // Deselect
                    } else {
                        question.userAnswers.push(selectedOptionLetter); // Select
                    }
                } else {
                    // If it's single choice, just replace the answer
                    question.userAnswers = [selectedOptionLetter];
                }
                
                updateScore();
                // We need to re-render the question to show the selection/feedback
                displayQuestion(currentQuestionIndex);
            }
            
            function showExplanationButton() {
                const explanationContainer = document.getElementById('explanation-container');
                if (explanationContainer.querySelector('.explanation-btn')) return;

                const question = questions[currentQuestionIndex];
                const button = document.createElement('button');
                button.textContent = 'Show Explanation';
                button.classList.add('explanation-btn', 'text-sm', 'bg-blue-500', 'hover:bg-blue-600', 'text-white', 'font-bold', 'py-2', 'px-4', 'rounded-lg');
                button.addEventListener('click', () => toggleExplanation(question));
                explanationContainer.appendChild(button);
            }

            function toggleExplanation(question) {
                const explanationContainer = document.getElementById('explanation-container');
                let contentDiv = explanationContainer.querySelector('.explanation-content');
                
                if (contentDiv) {
                    contentDiv.classList.toggle('show');
                } else {
                    contentDiv = document.createElement('div');
                    contentDiv.classList.add('explanation-content', 'mt-4', 'p-4', 'bg-gray-50', 'border', 'rounded-lg');
                    
                    let wrongExplanationHtml = question.wrongExplanation.replace(/\*\*(.*?)\*\*/g, '<strong class="text-red-600">$1</strong>');

                    contentDiv.innerHTML = `
                        <h4 class="font-bold text-lg mb-2 text-green-700">Explanation</h4>
                        <p class="mb-4 whitespace-pre-wrap">${question.explanation}</p>
                        ${question.wrongExplanation ? `<h4 class="font-bold text-lg mb-2 text-red-700">Why the others are wrong</h4><p class="whitespace-pre-wrap">${wrongExplanationHtml}</p>` : ''}
                    `;
                    explanationContainer.appendChild(contentDiv);
                    // Use a timeout to allow the element to be added to the DOM before adding the 'show' class for the transition to work
                    setTimeout(() => contentDiv.classList.add('show'), 10);
                }
            }

            function updateScore() {
                let correctCount = 0;
                questions.forEach(q => {
                    if (q.userAnswers.length > 0) {
                        const sortedUserAnswers = [...q.userAnswers].sort();
                        const sortedCorrectAnswers = [...q.correctAnswers].sort();
                        if (JSON.stringify(sortedUserAnswers) === JSON.stringify(sortedCorrectAnswers)) {
                            correctCount++;
                        }
                    }
                });
                scoreEl.textContent = `Score: ${correctCount} / ${questions.length}`;
            }

            function updateNavigation() {
                prevBtn.disabled = currentQuestionIndex === 0;
                nextBtn.disabled = currentQuestionIndex === questions.length - 1;
                questionSelect.value = currentQuestionIndex;
            }
            
            function populateQuestionSelect() {
                questionSelect.innerHTML = '';
                questions.forEach((q, index) => {
                    const option = document.createElement('option');
                    option.value = index;
                    option.textContent = `Question ${q.number}`;
                    questionSelect.appendChild(option);
                });
            }

            function resetQuiz() {
                questions.forEach(q => q.userAnswers = []);
                currentQuestionIndex = 0;
                updateScore();
                displayQuestion(currentQuestionIndex);
            }

            // --- TIMER LOGIC ---
            function formatTime(seconds) {
                const h = Math.floor(seconds / 3600).toString().padStart(2, '0');
                const m = Math.floor((seconds % 3600) / 60).toString().padStart(2, '0');
                const s = (seconds % 60).toString().padStart(2, '0');
                return `${h}:${m}:${s}`;
            }

            function startTimer() {
                if (timerInterval) return;
                timerInterval = setInterval(() => {
                    timerSeconds++;
                    timerDisplay.textContent = formatTime(timerSeconds);
                }, 1000);
            }

            function stopTimer() {
                clearInterval(timerInterval);
                timerInterval = null;
            }

            function resetTimer() {
                stopTimer();
                timerSeconds = 0;
                timerDisplay.textContent = formatTime(timerSeconds);
            }

            // --- EVENT LISTENERS ---
            prevBtn.addEventListener('click', () => {
                if (currentQuestionIndex > 0) {
                    displayQuestion(currentQuestionIndex - 1);
                }
            });

            nextBtn.addEventListener('click', () => {
                if (currentQuestionIndex < questions.length - 1) {
                    displayQuestion(currentQuestionIndex + 1);
                }
            });

            questionSelect.addEventListener('change', (e) => {
                displayQuestion(parseInt(e.target.value));
            });

            resetQuizBtn.addEventListener('click', resetQuiz);

            startTimerBtn.addEventListener('click', startTimer);
            stopTimerBtn.addEventListener('click', stopTimer);
            resetTimerBtn.addEventListener('click', resetTimer);

            // --- INITIALIZATION ---
            initializeQuiz();
        });
    </script>
</body>
</html>
